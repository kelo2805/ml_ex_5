{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ee60cfcf",
      "metadata": {
        "id": "ee60cfcf"
      },
      "source": [
        "# **Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e969a61",
      "metadata": {
        "id": "0e969a61"
      },
      "source": [
        "# Iris Dataset\n",
        "\n",
        "Il dataset Iris è un classico dataset nell'apprendimento automatico e nella statistica, introdotto da Ronald Fisher nel 1936. È comunemente utilizzato per attività di classificazione e clustering.\n",
        "\n",
        "## Caratteristiche e Struttura\n",
        "- **Campioni**: 150 campioni di fiori iris.\n",
        "- **Features**:\n",
        "  - Lunghezza del sepalo (cm)\n",
        "  - Larghezza del sepalo (cm)\n",
        "  - Lunghezza del petalo (cm)\n",
        "  - Larghezza del petalo (cm)\n",
        "- **Classi (Etichette Target)**:\n",
        "  - *Iris-setosa*\n",
        "  - *Iris-versicolor*\n",
        "  - *Iris-virginica*\n",
        "\n",
        "Ogni classe è rappresentata da 50 campioni.\n",
        "\n",
        "## Caratteristiche Principali\n",
        "- **Balanced Dataset**: Ogni classe contiene lo stesso numero di campioni.\n",
        "- **Perfect for Beginners**: a sua semplicità e struttura ben definita lo rendono perfetto per scopi didattici.\n",
        "- **Separable Classes**:\n",
        "  - *Iris-setosa* è linearmente separabile dalle altre due classi.\n",
        "  - *Iris-versicolor* e *Iris-virginica* sono più difficili da separare tra loro.\n",
        "\n",
        "\n",
        "# Iris Dataset Classes\n",
        "\n",
        "<table>\n",
        "    <tr>\n",
        "        <th>Iris Setosa</th>\n",
        "        <th>Iris Versicolor</th>\n",
        "        <th>Iris Virginica</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSFn-u9Lagrv8pV4zJ8Z1cEqXNL_uo39CrL6A&s\" alt=\"Iris setosa\" width=\"300\" height=\"300\"></td>\n",
        "        <td><img src=\"https://www.waternursery.it/document/img_prodotti/616/1646318149.jpeg\" alt=\"Iris versicolor\" width=\"300\" height=\"300\"></td>\n",
        "        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSQbTwTLA7_7SeTE3B1QOKw0TlB8Rp6NU7vyg&s\" alt=\"Iris virginica\" width=\"300\" height=\"300\"></td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e83ad2f0",
      "metadata": {
        "id": "e83ad2f0"
      },
      "source": [
        "# **K-means (matematica)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a6201e6",
      "metadata": {
        "id": "7a6201e6"
      },
      "source": [
        "### NumPy: `np.linalg.norm`\n",
        "`np.linalg.norm` calcola la norma di un vettore, matrice o array. Viene spesso utilizzato per calcolare distanze o grandezze.\n",
        "\n",
        "**Sintassi**:\n",
        "```python\n",
        "np.linalg.norm(array, axis=None, ord=None)\n",
        "```\n",
        "\n",
        "**Parametri**:\n",
        "- `array`: Array di input per cui viene calcolata la norma.\n",
        "- `axis`: Specifica l'asse lungo cui calcolare la norma. Se `None`, calcola la norma dell'intero array.\n",
        "- `ord`: Definisce il tipo di norma (es. 2 per norma Euclidea, 1 per norma Manhattan).\n",
        "\n",
        "**Esempio**:\n",
        "```python\n",
        "import numpy as np\n",
        "vector = np.array([3, 4])\n",
        "euclidean_norm = np.linalg.norm(vector)\n",
        "print(euclidean_norm)  # Output: 5.0 (distanza euclidea)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### NumPy: `np.allclose`\n",
        "`np.allclose` verifica se due array sono uguali elemento per elemento entro una certa tolleranza.\n",
        "\n",
        "**Sintassi**:\n",
        "```python\n",
        "np.allclose(array1, array2, rtol=1e-05, atol=1e-08)\n",
        "```\n",
        "\n",
        "**Parametri**:\n",
        "- `array1`, `array2`: Array da confrontare.\n",
        "- `rtol`: Tolleranza relativa.\n",
        "- `atol`: Tolleranza assoluta.\n",
        "\n",
        "**Ritorna**:\n",
        "- `True` se tutti gli elementi sono entro la tolleranza, altrimenti `False`.\n",
        "\n",
        "**Example**:\n",
        "```python\n",
        "import numpy as np\n",
        "array1 = np.array([1.0, 2.0, 3.0])\n",
        "array2 = np.array([1.0, 2.001, 3.0])\n",
        "is_close = np.allclose(array1, array2, atol=0.01)\n",
        "print(is_close)  # Output: True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b12be7d",
      "metadata": {
        "id": "2b12be7d"
      },
      "source": [
        "## **Esercizio 1: Implementare algoritmo K-means**\n",
        "\n",
        "Nel primo esercizio vi è richiesto di implementare l' algoritmo K-means. Di seguito troviamo una guida degli step da seguire:\n",
        "\n",
        "**1.** Loading dei dati e standardizzazione.\n",
        "\n",
        "**2.** Applicare PCA per ridurre a 2 dimensioni.\n",
        "\n",
        "**3.** Impostare parametri per K-means, cioè numero k di clusters, massimo numero di iterazioni e un seed.\n",
        "\n",
        "**4.** Inizializzazione dei centroidi.\n",
        "\n",
        "**5.** Iterare l' algoritmo.\n",
        "\n",
        "### **Algoritmo K-means**\n",
        "\n",
        "Per iterare l' algoritmo k-means ricordiamo che esso segue degli step ben precisi:\n",
        "\n",
        "**1.** **Assegnazone dei punti ai clusters**, calcolando la distanza tra ogni punto e i centroidi. Un punto verrà assegnato al centroide più vicino.\n",
        "\n",
        "**2.** **Aggiornamento centroidi.** Modifico la posizione dei centroidi con la media dei punti che sono stati assegnati a quel cluster.\n",
        "\n",
        "**3.** **Verifica convergenza**, misurando quanto distano i nuovi centroidi da quelli vecchi. Se questa differenza soddisfa una certa soglia (tolleranza) allora siamo giunti a convergenza. N.B. Se la convergenza non è raggiunta si continuerà ad iterare. Per continuare ad iterare devo però sostituire i nuovi centroidi.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af0ad3c4",
      "metadata": {
        "id": "af0ad3c4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. PREPARAZIONE DATI\n",
        "# Carica il dataset Iris\n",
        "iris = load_iris()\n",
        "X = iris.data    # Matrice dei dati: 150 fiori × 4 caratteristiche\n",
        "y = iris.target  # Etichette vere (non usate nel clustering)\n",
        "\n",
        "# Standardizza i dati per dare uguale peso a tutte le caratteristiche\n",
        "\n",
        "# svolgimento...\n",
        "\n",
        "# Applica PCA per ridurre a 2 dimensioni\n",
        "\n",
        "# svolgimento...\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data    # dati (150 campioni, 4 caratteristiche)\n",
        "y = iris.target  # etichette vere (non usate nel clustering)\n",
        "\n",
        "# Standardizza i dati\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Applica PCA per ridurre a 2 dimensioni\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aec2d4e",
      "metadata": {
        "id": "7aec2d4e"
      },
      "outputs": [],
      "source": [
        "# 2. IMPOSTAZIONE PARAMETRI\n",
        "\n",
        "np.random.seed(42)  # Per riproducibilità\n",
        "k = 3               # Numero di cluster desiderati\n",
        "max_iters = 20     # Numero massimo di iterazioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4901a78a",
      "metadata": {
        "id": "4901a78a"
      },
      "outputs": [],
      "source": [
        "# 3. INIZIALIZZAZIONE CENTROIDI\n",
        "# Seleziona k punti casuali dal dataset come centroidi iniziali\n",
        "\n",
        "# svolgimento...\n",
        "initial_indices = np.random.choice(X_pca.shape[0], k, replace=False)\n",
        "centroids = X_pca[initial_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b65d7612",
      "metadata": {
        "id": "b65d7612"
      },
      "outputs": [],
      "source": [
        "# 4. ALGORITMO K-MEANS\n",
        "for iterazione in range(max_iters):\n",
        "    # FASE 1: Assegnazione punti ai cluster\n",
        "    # Calcola la distanza di ogni punto da ogni centroide\n",
        "\n",
        "    # svolgimento...\n",
        "\n",
        "    distances = np.linalg.norm(X_pca[:, np.newaxis] - centroids, axis=2)\n",
        "    labels = np.argmin(distances, axis=1)\n",
        "\n",
        "\n",
        "    # Assegna ogni punto al centroide più vicino\n",
        "\n",
        "    # FASE 2: Aggiornamento centroidi\n",
        "    # Calcola i nuovi centroidi come la media dei punti assegnati a ciascun cluster\n",
        "\n",
        "    # svolgimento...\n",
        "    new_centroids = np.array([X_pca[labels == i].mean(axis=0) for i in range(k)])\n",
        "\n",
        "\n",
        "\n",
        "    # FASE 3: Verifica convergenza\n",
        "    # Calcola se i centroidi si sono spostati significativamente\n",
        "\n",
        "    # svolgimento...\n",
        "\n",
        "\n",
        "    if np.allclose(centroids, new_centroids, rtol=1e-5, atol=1e-8):\n",
        "        print(f\"Convergenza raggiunta all'iterazione {iteration + 1}.\")\n",
        "        break\n",
        "    # Aggiorna i centroidi per la prossima iterazione\n",
        "    centroids = new_centroids\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94fadffd",
      "metadata": {
        "id": "94fadffd"
      },
      "source": [
        "### **Visualizzazione**\n",
        "\n",
        "Utilizzate la seguente funzione per visualizzare il risultato dell' algoritmo. La funzione richiede 3 parametri:\n",
        "\n",
        "**1.** I dati su cui è stato applicato l' algoritmo di clustering.\n",
        "\n",
        "**2.** Le labels all' ultima iterazione dell' algoritmo.\n",
        "\n",
        "**3.** I centroidi all' ultima iterazione dell' aloritmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4b7ce5",
      "metadata": {
        "id": "0a4b7ce5"
      },
      "outputs": [],
      "source": [
        "def visualizza_clusters(X, labels, centroids):\n",
        "    \"\"\"\n",
        "    Visualizza i risultati del clustering.\n",
        "\n",
        "    Parametri:\n",
        "    X: array dei dati (standardizzati)\n",
        "    labels: array delle etichette dei cluster\n",
        "    centroids: array dei centroidi\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    colori = ['red', 'green', 'blue']\n",
        "\n",
        "    for i in range(len(np.unique(labels))):\n",
        "        mask = labels == i\n",
        "        plt.scatter(X[mask, 0], X[mask, 1],\n",
        "                   c=colori[i],\n",
        "                   label=f'Cluster {i}',\n",
        "                   alpha=0.6)\n",
        "\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                c='black',\n",
        "                marker='x',\n",
        "                s=200,\n",
        "                label='Centroidi')\n",
        "\n",
        "    plt.xlabel(\"Prima caratteristica\")\n",
        "    plt.ylabel(\"Seconda caratteristica\")\n",
        "    plt.title(\"Risultati K-means\")\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5bed57",
      "metadata": {
        "id": "7d5bed57"
      },
      "outputs": [],
      "source": [
        "# Chiamare la funzione visualizza clusters\n",
        "\n",
        "# svolgimento...\n",
        "\n",
        "visualizza_clusters(X_pca, labels, centroids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab9f3087",
      "metadata": {
        "id": "ab9f3087"
      },
      "source": [
        "## **Esercizio 2: Valutare l' algoritmo di clustering**\n",
        "\n",
        "Scriviamo una funzione che utilizzi diverse metriche per valutare l' algoritmo di clustering. ATTENZIONE: questa funzione potrà essere riutilizzata più avanti anche per altri algoritmi.\n",
        "\n",
        "Le metriche da implementare sono le seguenti:\n",
        "\n",
        "* **Silhouette score**\n",
        "\n",
        "* **Davies Bouldin score**\n",
        "\n",
        "* **Rand Index**\n",
        "\n",
        "* **Adjusted Rand Index**\n",
        "\n",
        "Per implementarle utilizzeremo direttamente le loro implementazioni fornite da `sklearn`.\n",
        "\n",
        "## **Sintassi**\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "silhouette = silhouette_score(X, labels)\n",
        "dbs = davies_bouldin_score(X, labels) # entrambe le metriche richiedono i dati e le labels\n",
        "print(silhouette)\n",
        "print(dbs)\n",
        "```\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import rand_score, adjusted_rand_score\n",
        "rand = rand_score(y, labels)\n",
        "adj_rand = adjusted_rand_score(y, labels) # Queste due metriche invece richiedono le lables originali e quelle assegnate dall' algoritmo\n",
        "print(rand)\n",
        "print(adj_rand)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5da4284",
      "metadata": {
        "id": "f5da4284"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, rand_score, adjusted_rand_score\n",
        "\n",
        "# VALUTAZIONE DEL CLUSTERING\n",
        "def evaluate_clustering(labels, X, y):\n",
        "\n",
        "    silhouette = silhouette_score(X, labels)\n",
        "\n",
        "    dbi = davies_bouldin_score(X, labels)\n",
        "\n",
        "    rand = rand_score(y, labels)\n",
        "\n",
        "    adj_rand = adjusted_rand_score(y, labels)\n",
        "\n",
        "    return silhouette, dbi, rand, adj_rand\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0c2759f",
      "metadata": {
        "id": "b0c2759f"
      },
      "outputs": [],
      "source": [
        "silhouette, dbi, rand, adj_rand = evaluate_clustering(labels, X, y)\n",
        "\n",
        "print(f\"Silhouette Score: {silhouette:.4f}\")\n",
        "print(f\"Davies-Bouldin Index: {dbi:.4f}\")\n",
        "print(f\"Rand Index: {rand:.4f}\")\n",
        "print(f\"Adjusted Rand Index: {adj_rand:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5248cf5b",
      "metadata": {
        "id": "5248cf5b"
      },
      "source": [
        "## **Esercizio 3: Clustering al variare dei parametri**\n",
        "\n",
        "Vediamo adesso come variano le prestazioni del clustering al variare di due parametri: il numero di clusters e il numero di componenti di PCA.\n",
        "\n",
        "Per semplicità utilizzeremo l' implementazione di `sklearn` per l' algoritmo K-means.\n",
        "\n",
        "Alla fine bisognerà stampare la configurazione di parametri che ci fa ottenere le migliori prestazioni utilizzando l' Adjusted Rand Index come metrica.\n",
        "\n",
        "### **Sintassi**:\n",
        "```python\n",
        "kmeans = KMeans(n_clusters=n)\n",
        "labels = kmeans.fit_predict(data)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ce1d363",
      "metadata": {
        "id": "2ce1d363"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_pca_components = [2,3,4]\n",
        "n_clusters = [2,3,4]\n",
        "\n",
        "# svolgimento...\n",
        "# inizializzazione Variabili per memorizzare i migliori risultati\n",
        "best_ari = -1\n",
        "best_params = {}\n",
        "for n_pca in n_pca_components:\n",
        "    for k in n_clusters:\n",
        "        # Riduzione della dimensione tramite PCA\n",
        "        pca = PCA(n_components=n_pca)\n",
        "        X_pca = pca.fit_transform(X)\n",
        "\n",
        "        # Clustering con K-means\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "        # Calcola l'Adjusted Rand Index\n",
        "        ari = adjusted_rand_score(y, labels)\n",
        "\n",
        "        # Salva la configurazione migliore\n",
        "        if ari > best_ari:\n",
        "            best_ari = ari\n",
        "            best_params = {'n_pca': n_pca, 'n_clusters': k}\n",
        "\n",
        "# Stampa la configurazione migliore\n",
        "print(f\"Migliori parametri: {best_params}\")\n",
        "print(f\"Adjusted Rand Index: {best_ari:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "382477a5",
      "metadata": {
        "id": "382477a5"
      },
      "source": [
        "## **Visualizzazione**\n",
        "\n",
        "La seguente funzione di visualizzazione mostra la differenza tra i dati assegnati nei clusters e la loro effettiva divisione in classi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb34bd52",
      "metadata": {
        "id": "fb34bd52"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_clusters_vs_truth(X_2d, labels, y_true=None, cluster_centers=None,\n",
        "                          cluster_colors=None, class_colors=None,\n",
        "                          class_names=None, title_clusters=\"Risultati Clustering\", title_truth=\"Vere Classi\"):\n",
        "    \"\"\"\n",
        "    Visualizza affiancati (subplot 1x2) i dati clusterizzati e la distribuzione delle vere classi.\n",
        "\n",
        "    Parametri obbligatori minimi:\n",
        "        - X_2d: array bidimensionale (come PCA) con shape (n_samples, 2)\n",
        "        - labels: array delle etichette di cluster per ciascun punto\n",
        "        - y_true: array delle etichette vere (opzionale; se non fornito mostra solo clustering)\n",
        "        - cluster_centers: array centroidi (opzionale; se fornito li visualizza)\n",
        "        - cluster_colors: lista di colori per i cluster (opzionale)\n",
        "        - class_colors: lista di colori per le classi vere (opzionale)\n",
        "        - class_names: lista di nomi per le classi vere (opzionale)\n",
        "        - title_clusters, title_truth: titoli dei plot (opzionali)\n",
        "    \"\"\"\n",
        "    n_clusters = len(np.unique(labels))\n",
        "    if cluster_colors is None:\n",
        "        cmap = plt.get_cmap(\"tab10\")\n",
        "        cluster_colors = [cmap(i) for i in range(n_clusters)]\n",
        "    if y_true is not None:\n",
        "        n_classes = len(np.unique(y_true))\n",
        "        if class_colors is None:\n",
        "            cmap2 = plt.get_cmap(\"tab10\")\n",
        "            class_colors = [cmap2(i) for i in range(n_classes)]\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2 if y_true is not None else 1, figsize=(12, 5) if y_true is not None else (6, 5))\n",
        "    axs = np.array(axs).reshape(-1)\n",
        "\n",
        "    # Clustering plot\n",
        "    for i, cluster_label in enumerate(np.unique(labels)):\n",
        "        axs[0].scatter(X_2d[labels == cluster_label, 0], X_2d[labels == cluster_label, 1],\n",
        "                       color=cluster_colors[i], alpha=0.5, label=f\"Cluster {cluster_label}\")\n",
        "    if cluster_centers is not None:\n",
        "        axs[0].scatter(cluster_centers[:,0], cluster_centers[:,1], c='red', marker='x', s=100, label='Centroids')\n",
        "    axs[0].set_xlabel(\"Principal Component 1\")\n",
        "    axs[0].set_ylabel(\"Principal Component 2\")\n",
        "    axs[0].set_title(title_clusters)\n",
        "    axs[0].legend(loc='best')\n",
        "    axs[0].set_aspect('auto')\n",
        "\n",
        "    # True class plot\n",
        "    if y_true is not None:\n",
        "        for i, class_label in enumerate(np.unique(y_true)):\n",
        "            label_name = class_names[class_label] if class_names is not None else f\"Class {class_label}\"\n",
        "            axs[1].scatter(X_2d[y_true == class_label, 0], X_2d[y_true == class_label, 1],\n",
        "                           color=class_colors[i], alpha=0.5, label=label_name)\n",
        "        axs[1].set_xlabel(\"Principal Component 1\")\n",
        "        axs[1].set_ylabel(\"Principal Component 2\")\n",
        "        axs[1].set_title(title_truth)\n",
        "        axs[1].legend(loc='best')\n",
        "        axs[1].set_aspect('auto')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa8c38fd",
      "metadata": {
        "id": "fa8c38fd"
      },
      "outputs": [],
      "source": [
        "# Utilizzare la funzione plot_clusters_vs_truth per visualizzare i risultati\n",
        "\n",
        "# svolgimento...\n",
        "plot_clusters_vs_truth(X_pca, labels, y, cluster_centers=centroids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd404869",
      "metadata": {
        "id": "bd404869"
      },
      "source": [
        "## **Esercizio 4: Gaussian Mixture Models**\n",
        "`GaussianMixture` modella i dati utilizzando una miscela di distribuzioni gaussiane.\n",
        "Vogliamo valutare anche questo modello al variare di numero di componenti di PCA e numero di clusters.\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "gmm = GaussianMixture(n_components=n)\n",
        "labels = gmm.fit_predict(data)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1c414d8",
      "metadata": {
        "id": "c1c414d8"
      },
      "outputs": [],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "n_pca_components = [2,3,4]\n",
        "n_clusters = [2,3,4]\n",
        "\n",
        "# svolgimento...\n",
        "best_ari = -1\n",
        "best_params = {}\n",
        "\n",
        "for n_pca in n_pca_components:\n",
        "    for k in n_clusters:\n",
        "        # Riduzione della dimensione tramite PCA\n",
        "        pca = PCA(n_components=n_pca)\n",
        "        X_pca = pca.fit_transform(X)\n",
        "\n",
        "        # Clustering con Gaussian Mixture\n",
        "        gmm = GaussianMixture(n_components=k, random_state=42)\n",
        "        labels = gmm.fit_predict(X_pca)\n",
        "\n",
        "        # Calcola l'Adjusted Rand Index\n",
        "        ari = adjusted_rand_score(y, labels)\n",
        "\n",
        "        # Salva la configurazione migliore\n",
        "        if ari > best_ari:\n",
        "            best_ari = ari\n",
        "            best_params = {'n_pca': n_pca, 'n_clusters': k}\n",
        "\n",
        "# Stampa la configurazione migliore\n",
        "print(f\"Migliori parametri: {best_params}\")\n",
        "print(f\"Adjusted Rand Index: {best_ari:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e5d417",
      "metadata": {
        "id": "84e5d417"
      },
      "outputs": [],
      "source": [
        "# visualizza i risultati del clustering GMM\n",
        "\n",
        "# svolgimento...\n",
        "plot_clusters_vs_truth(X_pca, labels, y, cluster_centers=centroids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1753d524",
      "metadata": {
        "id": "1753d524"
      },
      "source": [
        "## **Esercizio 5: Agglomerative Clustering**\n",
        "\n",
        "Agglomerative Clustering utilizza un approccio gerarchico per raggruppare i punti dati. Il **linkage method** determina come viene calcolata la distanza tra i cluster quando vengono uniti.\n",
        "\n",
        "### Scikit-Learn: `AgglomerativeClustering`\n",
        "`AgglomerativeClustering` effettua clustering gerarchico con uno specifico metodo di **linkage**.\n",
        "\n",
        "**Syntax**:\n",
        "```python\n",
        "clustering = AgglomerativeClustering(n_clusters=n, linkage='method')\n",
        "labels = clustering.fit_predict(data)\n",
        "```\n",
        "\n",
        "Anche in questo caso dovrete implementare l' algoritmo e valutarlo al variare del numero di clusters e numero di componenti della PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "258574cb",
      "metadata": {
        "id": "258574cb"
      },
      "source": [
        "### **Esercizio 5.1: Single Linkage**\n",
        "\n",
        "La distanza tra due cluster è definita come la distanza minima tra qualsiasi coppia di punti appartenenti ai cluster (vicino più prossimo).\n",
        "\n",
        "#### Vantaggi\n",
        "\n",
        "* Rapido ed efficiente per dataset di grandi dimensioni.\n",
        "\n",
        "* Utile per rilevare cluster allungati o di forma irregolare.\n",
        "\n",
        "#### Svantaggi\n",
        "\n",
        "* Può causare effetti di \"concatenazione\", collegando i cluster in sequenza anziché formare gruppi compatti.\n",
        "\n",
        "#### Formula\n",
        "$$ d_{single}(C_1, C_2) = \\min_{x \\in C_1, y \\in C_2} ||x - y|| $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3cec53",
      "metadata": {
        "id": "0d3cec53"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "\n",
        "n_pca_components = [2, 3, 4]\n",
        "n_clusters = [2, 3, 4]\n",
        "\n",
        "# svolgimento...\n",
        "\n",
        "\n",
        "\n",
        "# Tracciamento dei risultati\n",
        "best_ari = -1\n",
        "best_config = None\n",
        "\n",
        "for n_comp in n_pca_components:\n",
        "    # PCA\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    for k in n_clusters:\n",
        "        # Agglomerative Clustering con single linkage\n",
        "        clustering = AgglomerativeClustering(n_clusters=k, linkage='single')\n",
        "        labels = clustering.fit_predict(X_pca)\n",
        "\n",
        "        # Calcolo ARI\n",
        "        ari = adjusted_rand_score(y, labels)\n",
        "        print(f\"PCA components: {n_comp}, Clusters: {k}, ARI: {ari:.4f}\")\n",
        "\n",
        "        # Aggiorna il best score\n",
        "        if ari > best_ari:\n",
        "            best_ari = ari\n",
        "            best_config = (n_comp, k)\n",
        "\n",
        "print(f\"\\nMiglior configurazione: PCA = {best_config[0]}, Cluster = {best_config[1]}, ARI = {best_ari:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b5c4617",
      "metadata": {
        "id": "3b5c4617"
      },
      "outputs": [],
      "source": [
        "# visualizzare i risultati del clustering gerarchico\n",
        "\n",
        "# svolgimento...\n",
        "plot_clusters_vs_truth(X_pca, labels, y, cluster_centers=centroids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "739571ab",
      "metadata": {
        "id": "739571ab"
      },
      "source": [
        "### **Esercizio 5.2: Complete Linkage**\n",
        "La distanza tra due cluster è definita come la distanza massima tra qualsiasi coppia di punti appartenenti ai clusters (vicino più lontano).\n",
        "\n",
        "#### Vantaggi\n",
        "\n",
        "* **Cluster più compatti:** Produce gruppi densi e ben separati.\n",
        "\n",
        "* **Minore concatenamento:** Riduce l'effetto di \"catena\" tipico del Single Linkage.\n",
        "\n",
        "#### Svantaggi\n",
        "\n",
        "* **Sensibilità agli outlier:** Essendo basato sulle distanze massime, valori anomali influenzano fortemente i risultati.\n",
        "\n",
        "* **Dipendenza dalla scala:** Richiede una buona standardizzazione delle feature.\n",
        "\n",
        "\n",
        "#### Formula\n",
        "$$ d_{complete}(C_1, C_2) = \\max_{x \\in C_1, y \\in C_2} ||x - y|| $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c8dc246",
      "metadata": {
        "id": "9c8dc246"
      },
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "\n",
        "n_pca_components = [2, 3, 4]\n",
        "n_clusters = [2, 3, 4]\n",
        "\n",
        "# svolgimento...\n",
        "\n",
        "best_score = -1\n",
        "best_config = (None, None)\n",
        "\n",
        "for n_comp in n_pca_components:\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    for n in n_clusters:\n",
        "        clustering = AgglomerativeClustering(n_clusters=n, linkage='complete')\n",
        "        labels = clustering.fit_predict(X_pca)\n",
        "        score = adjusted_rand_score(y, labels)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_config = (n_comp, n)\n",
        "\n",
        "print(f\"Migliore configurazione (complete linkage): PCA={best_config[0]}, Cluster={best_config[1]}, ARI={best_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e94e50d5",
      "metadata": {
        "id": "e94e50d5"
      },
      "outputs": [],
      "source": [
        "# visualizzare i risultati del clustering gerarchico\n",
        "\n",
        "# svolgimento...\n",
        "plot_clusters_vs_truth(X_pca, labels, y, cluster_centers=centroids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6b6339f",
      "metadata": {
        "id": "f6b6339f"
      },
      "source": [
        "### **Esercizio 5.3: Average Linkage**\n",
        "La distanza tra due cluster è definita come la distanza media tra tutte le coppie di punti appartenenti ai diversi cluster.\n",
        "\n",
        "#### Vantaggi\n",
        "\n",
        "* **Bilanciamento ottimale:** Mitiga l'impatto degli outlier e delle dimensioni dei cluster\n",
        "\n",
        "* **Cluster più naturali:** Produce raggruppamenti spesso più coerenti con la struttura reale dei dati.\n",
        "\n",
        "#### Svantaggi\n",
        "\n",
        "* **Costo computazionale elevato:** Richiede il calcolo di tutte le distanze pairwise, diventando proibitivo per dataset molto grandi.\n",
        "\n",
        "* **Sensibilità alla densità:** Può essere influenzato da cluster con densità disomogenee.\n",
        "\n",
        "\n",
        "#### Formula\n",
        "$$ d_{average}(C_1, C_2) = \\frac{1}{|C_1| \\cdot |C_2|} \\sum_{x \\in C_1, y \\in C_2} ||x - y|| $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a848e0",
      "metadata": {
        "id": "33a848e0"
      },
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "\n",
        "n_pca_components = [2, 3, 4]\n",
        "n_clusters = [2, 3, 4]\n",
        "\n",
        "# svolgimento...\n",
        "\n",
        "best_score = -1\n",
        "best_config = (None, None)\n",
        "\n",
        "for n_comp in n_pca_components:\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    for n in n_clusters:\n",
        "        clustering = AgglomerativeClustering(n_clusters=n, linkage='average')\n",
        "        labels = clustering.fit_predict(X_pca)\n",
        "        score = adjusted_rand_score(y, labels)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_config = (n_comp, n)\n",
        "\n",
        "print(f\"Migliore configurazione (average linkage): PCA={best_config[0]}, Cluster={best_config[1]}, ARI={best_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80a6569e",
      "metadata": {
        "id": "80a6569e"
      },
      "outputs": [],
      "source": [
        "# visualizzare i risultati del clustering gerarchico\n",
        "\n",
        "# svolgimento...\n",
        "plot_clusters_vs_truth(X_pca, labels, y, cluster_centers=centroids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6441ea48",
      "metadata": {
        "id": "6441ea48"
      },
      "source": [
        "### **Esercizio 5.4: Ward Linkage**\n",
        "Unisce i cluster la cui fusione determina il minimo incremento della varianza totale intra-cluster (minimizza la somma dei quadrati degli scarti).\n",
        "\n",
        "#### Vantaggi\n",
        "\n",
        "* **Cluster compatti e sferici:** Ideale per raggruppamenti di forma regolare.\n",
        "\n",
        "* **Ottimizzazione della varianza:** Massimizza l'omogeneità interna ai cluster.\n",
        "\n",
        "#### Svantaggi\n",
        "\n",
        "* **Elevato costo computazionale:** Poco efficiente su dataset di grandi dimensioni.\n",
        "\n",
        "* **Ipotesi restrittive:** Assume una struttura di cluster isotropica.\n",
        "\n",
        "\n",
        "#### Formula\n",
        "$$ d_{ward}(C_1, C_2) = \\Delta \\text{variance} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e417920e",
      "metadata": {
        "id": "e417920e"
      },
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "\n",
        "n_pca_components = [2, 3, 4]\n",
        "n_clusters = [2, 3, 4]\n",
        "\n",
        "# svolgimento...\n",
        "\n",
        "best_score = -1\n",
        "best_config = (None, None)\n",
        "\n",
        "for n_comp in n_pca_components:\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    for n in n_clusters:\n",
        "        clustering = AgglomerativeClustering(n_clusters=n, linkage='ward')\n",
        "        labels = clustering.fit_predict(X_pca)\n",
        "        score = adjusted_rand_score(y, labels)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_config = (n_comp, n)\n",
        "\n",
        "print(f\"Migliore configurazione (average linkage): PCA={best_config[0]}, Cluster={best_config[1]}, ARI={best_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcae4c8f",
      "metadata": {
        "id": "bcae4c8f"
      },
      "outputs": [],
      "source": [
        "# visualizza i risultati del clustering gerarchico\n",
        "\n",
        "# svolgimento...\n",
        "plot_clusters_vs_truth(X_pca, labels, y, cluster_centers=centroids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd645e9f",
      "metadata": {
        "id": "cd645e9f"
      },
      "source": [
        "# **Esercizio 6: Testare tutti i linkage method**\n",
        "\n",
        "Infine dovrete testare contemporaneamente tutti i metod per ottenere tra tutti quello che raggiunge la performance migliore. Dovrete inoltre trovare la configurazione migliore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b92b4704",
      "metadata": {
        "id": "b92b4704"
      },
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "\n",
        "n_pca_components = [2, 3, 4]\n",
        "n_clusters = [2, 3, 4]\n",
        "linkage_methods = ['average', 'single', 'complete', 'ward']\n",
        "\n",
        "\n",
        "# svolgimento...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Per salvare il risultato migliore\n",
        "best_score = -1\n",
        "best_config = None\n",
        "\n",
        "for n_comp in n_pca_components:\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    for n in n_clusters:\n",
        "        # KMeans\n",
        "        kmeans = KMeans(n_clusters=n, random_state=42)\n",
        "        labels = kmeans.fit_predict(X_pca)\n",
        "        score = adjusted_rand_score(y, labels)\n",
        "        print(f\"KMeans - PCA: {n_comp}, Cluster: {n} → ARI = {score:.4f}\")\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_config = ('KMeans', n_comp, n, score)\n",
        "\n",
        "        # Gaussian Mixture\n",
        "        gmm = GaussianMixture(n_components=n, random_state=42)\n",
        "        labels = gmm.fit_predict(X_pca)\n",
        "        score = adjusted_rand_score(y, labels)\n",
        "        print(f\"GMM - PCA: {n_comp}, Cluster: {n} → ARI = {score:.4f}\")\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_config = ('GMM', n_comp, n, score)\n",
        "\n",
        "        # Agglomerative Clustering\n",
        "        for method in linkage_methods:\n",
        "            # Ward linkage richiede almeno 2 features (quindi min n_comp=2)\n",
        "            if method == 'ward' and n_comp < 2:\n",
        "                continue\n",
        "            agglo = AgglomerativeClustering(n_clusters=n, linkage=method)\n",
        "            labels = agglo.fit_predict(X_pca)\n",
        "            score = adjusted_rand_score(y, labels)\n",
        "            print(f\"Agglomerative ({method}) - PCA: {n_comp}, Cluster: {n} → ARI = {score:.4f}\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_config = (f'Agglomerative-{method}', n_comp, n, score)\n",
        "\n",
        "print(\"\\n Miglior configurazione trovata:\")\n",
        "print(f\"Metodo: {best_config[0]}, PCA: {best_config[1]}, Cluster: {best_config[2]} → ARI = {best_config[3]:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "best_method, best_pca, best_k, best_ari = best_config\n",
        "pca = PCA(n_components=best_pca)\n",
        "X_pca_best = pca.fit_transform(X)\n",
        "\n",
        "# Ricostruzione clustering con metodo migliore\n",
        "if best_method == 'KMeans':\n",
        "    model = KMeans(n_clusters=best_k, random_state=42)\n",
        "    labels = model.fit_predict(X_pca_best)\n",
        "    centroids = model.cluster_centers_\n",
        "\n",
        "elif best_method == 'GMM':\n",
        "    model = GaussianMixture(n_components=best_k, random_state=42)\n",
        "    labels = model.fit_predict(X_pca_best)\n",
        "    centroids = model.means_\n",
        "\n",
        "elif best_method.startswith('Agglomerative'):\n",
        "    linkage_type = best_method.split('-')[1]\n",
        "    model = AgglomerativeClustering(n_clusters=best_k, linkage=linkage_type)\n",
        "    labels = model.fit_predict(X_pca_best)\n",
        "    centroids = None  # Agglomerative clustering non fornisce i centroidi\n",
        "\n",
        "# Visualizzazione finale\n",
        "plot_clusters_vs_truth(X_pca_best, labels, y, cluster_centers=centroids)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}